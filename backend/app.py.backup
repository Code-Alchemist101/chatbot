from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
from datetime import datetime, timezone
import threading
import time
import uuid
import json
from urllib.parse import urlparse
from logger import setup_logger
from rate_limiter import rate_limiter
from config import (
    FLASK_DEBUG, FLASK_PORT, MAX_CRAWL_HISTORY, 
    CRAWL_CLEANUP_INTERVAL, MIN_CRAWL_DEPTH, MAX_CRAWL_DEPTH, DEFAULT_CRAWL_DEPTH
)
from db import db
from rag import get_answer, get_answer_stream
from ingest import ingest

# Setup logging
logger = setup_logger(__name__)

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Global crawl status tracker
crawl_status = {}
url_to_crawl_id = {}  # Fix #1: Track URL -> crawl_id mapping
crawl_status_lock = threading.Lock()

def cleanup_old_crawls():
    """Fix #2: Cleanup old crawl statuses to prevent memory leak"""
    while True:
        time.sleep(CRAWL_CLEANUP_INTERVAL)
        with crawl_status_lock:
            if len(crawl_status) > MAX_CRAWL_HISTORY:
                # Sort by completion time, keep most recent
                completed = [
                    (cid, status) for cid, status in crawl_status.items()
                    if status['status'] in ['completed', 'failed']
                ]
                completed.sort(key=lambda x: x[1].get('completed_at', ''), reverse=True)
                
                # Remove oldest completed crawls
                to_remove = completed[MAX_CRAWL_HISTORY:]
                for crawl_id, status in to_remove:
                    url = status.get('url')
                    if url and url_to_crawl_id.get(url) == crawl_id:
                        del url_to_crawl_id[url]
                    del crawl_status[crawl_id]
                
                logger.info(f"Cleaned up {len(to_remove)} old crawl statuses")
        
        # Also cleanup rate limiter
        rate_limiter.cleanup_old_keys()

# Start cleanup thread
cleanup_thread = threading.Thread(target=cleanup_old_crawls, daemon=True)
cleanup_thread.start()

@app.route('/api/health', methods=['GET'])
def health():
    """Improvement #2: Enhanced health check with service status"""
    status = {
        "status": "ok",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "services": {}
    }
    
    # Check MongoDB
    try:
        if db.collection is not None:
            db.client.admin.command('ping')
            status["services"]["mongodb"] = "connected"
        else:
            status["services"]["mongodb"] = "disabled"
    except Exception as e:
        status["services"]["mongodb"] = f"error: {str(e)}"
        status["status"] = "degraded"
    
    # Check Pinecone (via import check)
    try:
        from rag import vector_store
        status["services"]["pinecone"] = "connected"
    except Exception as e:
        status["services"]["pinecone"] = f"error: {str(e)}"
        status["status"] = "degraded"
    
    # Check Gemini (via import check)
    try:
        from rag import llm
        status["services"]["gemini"] = "configured"
    except Exception as e:
        status["services"]["gemini"] = f"error: {str(e)}"
        status["status"] = "degraded"
    
    return jsonify(status)

@app.route('/api/chat', methods=['POST'])
def chat():
    """Non-streaming chat endpoint with rate limiting"""
    data = request.json
    question = data.get('question')
    session_id = data.get('session_id', 'default')
    filters = data.get('filters', None)

    if not question:
        return jsonify({"error": "Question is required"}), 400

    if len(question.strip()) < 3:
        return jsonify({"error": "Question too short"}), 400

    if len(question) > 1000:
        return jsonify({"error": "Question too long (max 1000 characters)"}), 400

    # Fix #5: Rate limiting
    if not rate_limiter.is_allowed(session_id):
        return jsonify({
            "error": "Rate limit exceeded. Please wait before sending more requests.",
            "retry_after": 60
        }), 429

    try:
        # Save user message
        db.save_message(session_id, "user", question)

        # Generate answer
        answer = get_answer(question, filters=filters)

        if not answer or answer.strip() == "":
            answer = "I couldn't generate a response. Please try rephrasing your question."

        # Save AI message
    logger.info("Crawl endpoint called!")
    logger.info("="*60)
    
    data = request.json
    url = data.get('url')
    depth = data.get('depth', DEFAULT_CRAWL_DEPTH)
    
    if not url:
        return jsonify({"error": "URL is required"}), 400

    # Validate URL
    if not url.startswith(('http://', 'https://')):
        return jsonify({"error": "Invalid URL format"}), 400

    # Fix #11: Validate depth parameter
    if not isinstance(depth, int) or depth < MIN_CRAWL_DEPTH or depth > MAX_CRAWL_DEPTH:
        return jsonify({
            "error": f"Depth must be between {MIN_CRAWL_DEPTH} and {MAX_CRAWL_DEPTH}"
        }), 400

    with crawl_status_lock:
        # Fix #1: Check if URL is already being crawled
        if url in url_to_crawl_id:
            existing_crawl_id = url_to_crawl_id[url]
            existing_status = crawl_status.get(existing_crawl_id)
            
            if existing_status and existing_status['status'] == 'running':
                return jsonify({
                    "message": "This URL is already being crawled",
                    "crawl_id": existing_crawl_id,
                    "status_url": f"/api/crawl/status/{existing_crawl_id}",
                    "stream_url": f"/api/crawl/stream/{existing_crawl_id}"
                }), 200

        # Initialize crawl status
        crawl_id = str(uuid.uuid4())
        url_to_crawl_id[url] = crawl_id
        
        # Extract domain for namespace
        domain = urlparse(url).netloc.replace('www.', '')
        
        crawl_status[crawl_id] = {
            'url': url,
            'domain': domain,
            'depth': depth,
            'status': 'running',
            'started_at': datetime.utcnow().isoformat(),
            'progress': {
                'pages_crawled': 0,
                'pages_indexed': 0,
                'errors': 0,
                'stage': 'initializing'
            }
        }

    # Run ingestion in background
    def crawl_with_status(url, depth, crawl_id):
        try:
            result = ingest(url, depth, crawl_id, crawl_status)
            with crawl_status_lock:
                crawl_status[crawl_id]['status'] = 'completed'
                crawl_status[crawl_id]['completed_at'] = datetime.now(timezone.utc).isoformat()
                crawl_status[crawl_id]['result'] = result
            logger.info(f"Crawl {crawl_id} completed successfully")
        except Exception as e:
            with crawl_status_lock:
                crawl_status[crawl_id]['status'] = 'failed'
                crawl_status[crawl_id]['error'] = str(e)
                crawl_status[crawl_id]['completed_at'] = datetime.now(timezone.utc).isoformat()
            logger.error(f"Crawl {crawl_id} failed: {str(e)}", exc_info=True)

    thread = threading.Thread(target=crawl_with_status, args=(url, depth, crawl_id))
    thread.daemon = True
    thread.start()

    return jsonify({
        "message": f"Started crawling {url}",
        "crawl_id": crawl_id,
        "status_url": f"/api/crawl/status/{crawl_id}",
        "stream_url": f"/api/crawl/stream/{crawl_id}"
    })

@app.route('/api/crawl/status/<crawl_id>', methods=['GET'])
def crawl_status_endpoint(crawl_id):
    """Get crawl status by ID"""
    if crawl_id not in crawl_status:
        return jsonify({"error": "Crawl ID not found"}), 404
    
    with crawl_status_lock:
        return jsonify(crawl_status[crawl_id])

@app.route('/api/crawl/stream/<crawl_id>', methods=['GET'])
def crawl_stream(crawl_id):
    """Stream crawl status updates via SSE"""
    def generate():
        max_iterations = 600  # 5 minutes max (600 * 0.5s)
        iterations = 0
        
        while iterations < max_iterations:
            if crawl_id not in crawl_status:
                yield f"data: {json.dumps({'error': 'Crawl ID not found'})}\n\n"
                break
            
            with crawl_status_lock:
                data = crawl_status[crawl_id].copy()
            
            # Yield current status
            yield f"data: {json.dumps(data)}\n\n"
            
            if data['status'] in ['completed', 'failed']:
                break
            
            time.sleep(0.5)
            iterations += 1
        
        if iterations >= max_iterations:
            yield f"data: {json.dumps({'error': 'Stream timeout'})}\n\n"
            
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

@app.route('/api/crawl/active', methods=['GET'])
def active_crawls():
    """Get all active crawls"""
    with crawl_status_lock:
        active = {k: v for k, v in crawl_status.items() if v['status'] == 'running'}
    return jsonify({"active_crawls": active})

if __name__ == '__main__':
    logger.info(f"Starting Flask server on port {FLASK_PORT}")
    try:
        app.run(debug=FLASK_DEBUG, port=FLASK_PORT, threaded=True)
    finally:
        # Close database connection on shutdown
        db.close()